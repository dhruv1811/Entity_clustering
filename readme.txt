In probability theory and applications, Bayes' theorem (alternatively Bayes' law or Bayes' rule) links a conditional probability to its inverse. That is, it provides the relationship between P(A | B) and P(B | A). It is valid in all common interpretations of probability, and is commonly used in science and engineering.[1] The theorem is named for Thomas Bayes (pronounced /ˈbeɪz/ or "bays").[2]
Under the frequentist interpretation of probability, a probability measures how likely an event is to occur. On this view, Bayes' theorem is a general relationship between P(A), P(B), P(A | B) and P(B | A) for any events A and B in the same event space.
Under the Bayesian interpretation of probability, a probability, or uncertainty, measures how likely something is to be true. On this view, Bayes' theorem links the uncertainty of a probability model before and after observing the system being modelled. For example, a probability model, A, is hypothesised to represent a die with an unknown bias. The die is thrown a number of times to collect evidence, B. P(A), the prior, is the initial uncertainty in the model. P(A | B), the posterior, is the uncertainty in the model having accounted for whether the evidence supports or refutes the model. P(B | A) / P(B) represents the degree of support B provides for A. For more information on the application of Bayes' theorem under the Bayesian interpretation of probability, see Bayesian inference.
